\documentclass{article}
\usepackage[left = 3cm, right = 3cm]{geometry}

\begin{document}
\begin{Huge}
\noindent Sheng Xu
\end{Huge}

\vspace{5mm}
\begin{minipage}{0.7\linewidth}
  \flushleft
\noindent 24 Hillhouse Avenue \\
\noindent New Haven, CT 06511
\end{minipage}
\hfill
\begin{minipage}{0.29\linewidth}
  Phone: 410-868-9529 \\
  Email: sheng.xu@yale.edu
\end{minipage}

\vspace{5mm}
\begin{large}
\noindent \textbf{Education}
\end{large}
\vspace{5mm}

\begin{minipage}{0.65\linewidth}
\noindent Ph.D. Statistics, Yale University.\\
Advisors: Zhou Fan and Sahand Negahban.\\

\noindent B.S. Probability and Statistics, Peking University. \\
Advisor: Zhi Geng.\\

\noindent B.S. (dual) Economics, Peking University. \\
Advisor: Miaojie Yu.\\
\end{minipage}\hfill
\begin{minipage}{0.34\linewidth}
\flushright
Sep. 2016 - Jul. 2022 (Expected)  \\
~\\
~\\
Sep. 2010 - Jul. 2014\\
~\\
~\\
Sep. 2011 - Jul. 2014\\
\end{minipage}




% \vspace{2mm}
% Lecturer, Complementary Session for Statistics 210B, Spring 2016.

% \hspace{4mm} Self-prepared weekly lectures, covering learning theories, concentration inequalities, 

% \hspace{4mm} high dimensional geometry, sparse linear regression, compressed sensing and kernel theories.

\vspace{5mm}
\begin{large}
\noindent \textbf{Research Experiences}
\end{large}
\vspace{5mm}

\begin{large}
\noindent Research Interests
\end{large}

\vspace{3mm}



High Dimensional and Multivariate Statistics; Statistical Inference; Signal and Image Processing; Combinatorial Algorithms; Convex and Nonconvex Optimization; Cryo-EM; Deep Learning Theory; Network Analysis; Time Series Analysis; Econometrics.

\vspace{3mm}

\begin{large}
\noindent Journal Publications
\end{large}

\begin{enumerate}
\item \textbf{Xu, S.}, \& Fan, Z. (2021). Iterative Alpha Expansion for Estimating Gradient-Sparse Signals from Linear Measurements. \emph{Journal of the Royal Statistical Society: Series B (JRSS-B)}.
\item Han, F., \textbf{Xu, S.}, \& Zhou, W. (2018). On Gaussian Comparison Inequality and Its Application to Spectral Analysis of Large Random Matrices. \emph{Bernoulli}, Volume 24, Number 3 (2018), 1787-1833.
\end{enumerate}


\begin{large}
\noindent Conference Publications
\end{large}

\begin{enumerate}
\item Chen, L., \& \textbf{Xu, S.} (2021). Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS. In \emph{Proceedings of the 9th International Conference on Learning Representations (ICLR)}.
\item \textbf{Xu, S.}, Fan, Z., \& Negahban, S. (2020). Tree-Projected Gradient Descent for Estimating Gradient-Sparse Parameters on Graphs.  In \emph{Proceedings of the 33rd Annual Conference on Learning Theory (COLT)}.
\item Qiu, H., \textbf{Xu, S.}, Han, F., Liu, H., \& Caffo, B. (2015). Robust Estimation of Transition Matrices in High Dimensional Heavy-Tailed Vector Autoregressive Processes. In \emph{Proceedings of the 32nd International
Conference on Machine Learning (ICML)}.
\end{enumerate}



\begin{large}
\noindent Preprints
\end{large}

\begin{enumerate}

\item Fan, Z., Lederman, R., Sun, Y., Wang, T., \& \textbf{Xu, S.} (2021). Maximum Likelihood for High-Noise Group Orbit Estimation and Single-Particle Cryo-EM. Submitted to \emph{Annals of Statistics}.

\item Gao, W. Y., Li, M., \& \textbf{Xu, S.} (2021). Logical Differencing in Dyadic Network Formation Models with Nontransferable Utilities. Under revision at \emph{Journal of Econometrics}.

\item Gao, W. Y., \& \textbf{Xu, S.} (2020). Two-Stage Maximum Score Estimator. \emph{ArXiv e-prints, abs/2009.02854}.
\end{enumerate}





\vspace{5mm}
\begin{large}
\noindent \textbf{Teaching Experiences}
\end{large}
\vspace{5mm}


TA, S\&{DS} 363/563, Multivariate Statistics, Spring 2020 (taught by Jonathan, Reuning-Scherer). 

\vspace{2mm}
TA, S\&{DS} 410/610, Statistical Inference, Fall 2019 (taught by Zhou, Fan). 

\vspace{2mm}
TA, S\&{DS} 351/551, Stochastic Processes, Spring 2019 (taught by Sahand, Negahban and Yihong, Wu). 

\vspace{2mm}
TA, S\&{DS} 410/610, Statistical Inference, Fall 2018 (taught by Zhou, Fan). 

\vspace{2mm}
TA, S\&{DS} 351/551, Stochastic Processes, Spring 2018 (taught by Sahand, Negahban). 



\vspace{5mm}
\begin{large}
\noindent \textbf{Professional Services}
\end{large}
\vspace{5mm}

\begin{large}
\noindent Talks
\end{large}


\vspace{2mm}
Conference on Learning Theory (COLT), 2020.

\vspace{2mm}
International Conference on Machine Learning (ICML), 2015.



\vspace{2mm}

\begin{large}
\noindent Reviewing
\end{large}


\vspace{2mm}
International Conference on Machine Learning (ICML), 2020

\vspace{2mm}
International Conference on Learning Representations (ICLR), 2021

\vspace{2mm}
Conference on Neural Information Processing Systems (NeurIPS), 2021

% \vspace{5mm}
% \begin{large}
% \noindent \textbf{Courses}
% \end{large}

% \begin{enumerate}
% \item Fall, 2014: STAT205A (Probability Theory): A+; STAT210A (Theoretical Statistics): A+; 

% \hspace{1.62cm} STAT215A (Applied Statistics): A+.
% \item Spring, 2015: STAT205B (Probability Theory): A+; STAT210B (Theoretical Statistics): A+; 

% \hspace{2.05cm} EE227C (Convex Optimization): A.
% \item Fall, 2015: EE229A (Information Theory and Coding): A+; 

% \hspace{1.62cm} STAT212A (Multiple Testing and Selective Inference): A+.
% \item Spring, 2016: EE229B (Error Control Coding): A.

% \item Fall, 2016: STAT241A (Statistical Learning): A; 

% \hspace{1.62cm} STAT240 (Nonparametric and Robust Methods): A+.
% \end{enumerate}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
